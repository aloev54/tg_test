#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
site_to_telegram.py ‚Äî –†—É–ª–Å–∂–∫–∞-—Å—Ç–∞–π–ª (LLM + –∞–Ω—Ç–∏-—Ä–µ–∫–ª–∞–º–∞)

–§–æ—Ä–º–∞—Ç –ø–æ—Å—Ç–∞:
- –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞: —ç–º–æ–¥–∑–∏ + –∂–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
- –¥–∞–ª–µ–µ: —Ç–µ–∫—Å—Ç —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã (–±–µ–∑ –ø—É–Ω–∫—Ç–æ–≤), —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ –∫—É—Ä—Å–∏–≤–µ
- –ù–ò–ö–ê–ö–ò–• —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
- –ø–æ–¥–ø–∏—Å—å: üèéÔ∏è *–†—É–ª–Å–∂–∫–∞* (https://t.me/drive_hedgehog)
- –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ photo + HTML caption; –ø—Ä–∏–∫—Ä–µ–ø–ª—è–µ–º og:image

–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ:
- message_thread_id (–ø–æ—Å—Ç –≤ —Ç–µ–º—É –≤ –≥—Ä—É–ø–ø–µ)
- –∫–æ–ø–∏—è –≤ –¥—Ä—É–≥–æ–π —á–∞—Ç/–∫–∞–Ω–∞–ª (copyMessage)
- LLM-–ø–æ–ª–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ OPENAI_API_KEY
"""

import argparse
import html
import json
import os
import re
import sys
import time
from dataclasses import dataclass
from typing import List, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

DEFAULT_UA = "Mozilla/5.0 (compatible; rul-ezhka/1.6)"
TELEGRAM_API_BASE = "https://api.telegram.org"
STATE_FILE = "seen.json"

ARTICLE_SELECTORS = [
    "article",
    "[itemprop='articleBody']",
    ".article__body",
    ".js-mediator-article",
    ".article",
]
DROP_SELECTORS = [
    "script", "style", "noscript",
    "[class*='advert']", "[class*='ad-']", "[class*='ad_']", "[id*='ad']",
    "[class*='banner']", "[class*='promo']",
    "[class*='subscribe']", "[class*='subscription']",
    "[class*='breadcrumbs']", "[class*='share']", "[class*='social']",
    "[class*='tags']", "[class*='related']", "[class*='widget']",
    "figure figcaption", ".photo-credit", ".copyright",
]
DROP_PHRASES = [
    "—Ä–µ–∫–ª–∞–º–∞", "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å", "–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å", "–ø–æ–¥–ø–∏—Å–∫–∞",
    "—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ", "—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "—Ä–∞—Å—Å—ã–ª–∫–∞",
    "–Ω–∞—à —Ç–µ–ª–µ–≥—Ä–∞–º", "–Ω–∞—à telegram", "instagram", "vk.com", "–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ",
    "—Å–∫–∞—á–∞–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å –∫", "–ø—Ä–æ–º–æ–∫–æ–¥",
]

EMOJI_MAP = [
    (["–¥—Ç–ø", "–∞–≤–∞—Ä", "—Å—Ç–æ–ª–∫–Ω–æ–≤"], "üö®"),
    (["—à—Ç—Ä–∞—Ñ", "–Ω–∞–ª–æ–≥", "–ø–æ—à–ª–∏–Ω", "—É—Ç–∏–ª—å—Å–±–æ—Ä"], "üí∏"),
    (["—ç–ª–µ–∫—Ç—Ä–æ", "ev", "–±–∞—Ç–∞—Ä", "–∑–∞—Ä—è–¥"], "‚ö°"),
    (["–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª", "—Ç–æ–ø–ª–∏–≤"], "‚õΩ"),
    (["—Ç—Ä–∞—Å—Å", "–¥–æ—Ä–æ–≥", "—Ä–µ–º–æ–Ω—Ç", "–º–æ—Å—Ç"], "üõ£Ô∏è"),
    (["—Ç–µ—Å—Ç", "–æ–±–∑–æ—Ä", "—Ç–µ—Å—Ç-–¥—Ä–∞–π–≤"], "üß™"),
    (["–≥–æ–Ω–∫", "—Ç—Ä–µ–∫", "—Ä–∞–ª–ª–∏", "—Å–ø–æ—Ä—Ç"], "üèÅ"),
]


@dataclass
class Item:
    title: str
    url: str
    text: str            # –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç (—Å <i>–∫—É—Ä—Å–∏–≤–æ–º</i>), –±–µ–∑ –ø—É–Ω–∫—Ç–æ–≤
    image: Optional[str]


# ---------------- Core helpers ----------------
def fetch_html(url: str) -> str:
    r = requests.get(url, headers={"User-Agent": DEFAULT_UA}, timeout=25)
    r.raise_for_status()
    return r.text


def extract_listing_links(list_html: str, base_url: Optional[str], selector: str, limit: int) -> List[str]:
    soup = BeautifulSoup(list_html, "html.parser")
    nodes = soup.select(selector)[:limit]
    links = []
    for n in nodes:
        a = n if n.name == "a" else n.find("a")
        if a and a.get("href"):
            links.append(urljoin(base_url, a["href"]) if base_url else a["href"])
    # dedupe preserve order
    seen, out = set(), []
    for u in links:
        if u not in seen:
            out.append(u); seen.add(u)
    return out


def choose_emoji(title: str, text: str) -> str:
    s = (title + " " + text).lower()
    for keys, e in EMOJI_MAP:
        if any(k in s for k in keys):
            return e
    return "üöó"


def clean_text(t: str) -> str:
    t = re.sub(r"\s+", " ", t).strip()
    t = re.sub(r"–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ.*$", "", t, flags=re.I)
    return t


def is_junk(t: str) -> bool:
    lt = t.lower()
    if len(lt) < 40:
        return True
    return any(p in lt for p in DROP_PHRASES)


def split_sents(text: str) -> List[str]:
    parts = re.split(r"(?<=[.!?‚Ä¶])\s+", text)
    return [p.strip() for p in parts if p.strip()]


def medium_text(paras: List[str], target=700) -> str:
    # –°–æ–±–∏—Ä–∞–µ–º 1‚Äì3 –∞–±–∑–∞—Ü–∞ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã –±–µ–∑ –ø—É–Ω–∫—Ç–æ–≤
    sents = split_sents(" ".join(paras))
    out, cur = [], 0
    for s in sents:
        if is_junk(s):
            continue
        if cur + len(s) > target and out:
            break
        out.append(s); cur += len(s) + 1
        if len(out) >= 5:
            break
    text = " ".join(out).strip()
    if len(text) > target + 150:
        text = text[:target].rstrip() + "‚Ä¶"
    return text


def parse_article(url: str, base_url: Optional[str]) -> tuple[str, Optional[str], List[str]]:
    """return title, image, clean paragraphs list"""
    html_text = fetch_html(url)
    soup = BeautifulSoup(html_text, "html.parser")

    # title
    title = None
    t = soup.find("meta", property="og:title")
    if t and t.get("content"):
        title = t["content"].strip()
    if not title and soup.title and soup.title.string:
        title = soup.title.string.strip()
    title = title or url

    # image
    image = None
    im = soup.find("meta", property="og:image")
    if im and im.get("content"):
        image = im["content"].strip()
    if image and base_url:
        image = urljoin(base_url, image)

    # body
    root = None
    for sel in ARTICLE_SELECTORS:
        node = soup.select_one(sel)
        if node:
            root = node; break
    if not root:
        root = soup

    # drop junk nodes
    for sel in DROP_SELECTORS:
        for node in root.select(sel):
            node.decompose()

    # collect paragraphs
    paras = []
    for p in root.find_all("p"):
        txt = clean_text(p.get_text(" ", strip=True))
        if txt and not is_junk(txt):
            paras.append(txt)

    # Ensure we have some text
    if not paras:
        desc = soup.find("meta", property="og:description")
        if desc and desc.get("content"):
            paras = [desc["content"].strip()]

    return title, image, paras


def italicize_some_keywords(text: str, max_terms=4) -> str:
    """–ù–∞ —Å–ª—É—á–∞–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è LLM: –∫—É—Ä—Å–∏–≤ 2‚Äì4 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)."""
    words = re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9\-]{5,}", text)
    stop = set("–∫–æ—Ç–æ—Ä—ã–µ –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ç–æ—Ä–∞—è –∫–æ—Ç–æ—Ä–æ–µ —Ç–∞–∫–∂–µ –µ—Å–ª–∏ —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –º–µ–∂–¥—É –±–æ–ª–µ–µ –æ—á–µ–Ω—å —Ç–æ–≥–¥–∞ —á—Ç–æ–±—ã —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–¥ —Å–≤—è–∑–∏ —Å–≤–æ–µ–º —Å–≤–æ–µ–º —Å–≤–æ–µ–π —Å–≤–æ–∏—Ö –≤—Å–µ–≥–æ –º–æ–∂–µ—Ç –ø–æ–∫–∞ –ø–æ–∫–∞ –ª—é–±–æ–º –ª—é–±–æ–º —Ç–∞–∫–∏—Ö —Ç–∞–∫–∏–µ —Ç–∞–∫–∞—è —Ç–∞–∫–æ–µ –±—É–¥–µ—Ç –±—É–¥—É—Ç —Å—Ç–∞–ª–∏ —Å—Ç–æ–ª—å–∫–æ —Ç–∞–∫–æ–π —Ç–∞–∫–∏—Ö —ç—Ç–∏—Ö —ç—Ç–∏–º —ç—Ç–æ–º".split())
    # –í—ã–±–∏—Ä–∞–µ–º ¬´—á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è¬ª –∏ –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    freq = {}
    for w in words:
        lw = w.lower()
        if lw in stop:
            continue
        freq[lw] = freq.get(lw, 0) + 1
    terms = sorted(freq, key=freq.get, reverse=True)[:max_terms]
    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
    def repl(m):
        w = m.group(0)
        lw = w.lower()
        if lw in terms and not hasattr(repl, "used") or lw not in getattr(repl, "used", set()):
            repl.used = getattr(repl, "used", set()); repl.used.add(lw)
            return f"<i>{w}</i>"
        return w
    return re.sub(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9\-]{5,}", repl, text, count=0)


# ---------------- LLM ‚Äú–ø–æ–≤–∞—Ä‚Äù ----------------
def llm_make_text(title: str, merged_text: str) -> Optional[str]:
    """
    –ï—Å–ª–∏ –∑–∞–¥–∞–Ω OPENAI_API_KEY ‚Äî –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å:
    - –≤—ã–¥–∞—Ç—å 1‚Äì3 –∞–±–∑–∞—Ü–∞ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã –±–µ–∑ –ø—É–Ω–∫—Ç–æ–≤
    - –≤—ã–¥–µ–ª–∏—Ç—å 2‚Äì4 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ –ö–£–†–°–ò–í–û–ú —Å —Ç–µ–≥–∞–º–∏ <i>‚Ä¶</i>
    - –Ω–µ –≤—Å—Ç–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏ –∏ –ø—Ä–∏–∑—ã–≤—ã
    - –±–µ–∑ HTML –∫—Ä–æ–º–µ <i>
    """
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        return None
    try:
        import json as _json, urllib.request as _url
        payload = {
            "model": "gpt-4o-mini",
            "temperature": 0.3,
            "messages": [
                {"role": "system", "content": "–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –Ω–∞–ø–∏—Å–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π –∞–≤—Ç–æ–Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç. –§–æ—Ä–º–∞—Ç: 1‚Äì3 –∞–±–∑–∞—Ü–∞ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã. –ë–µ–∑ –ø—É–Ω–∫—Ç–æ–≤/—Å–ø–∏—Å–∫–æ–≤. –ë–µ–∑ —Å—Å—ã–ª–æ–∫. –ò—Å–ø–æ–ª—å–∑—É–π –∫—É—Ä—Å–∏–≤ —É 2‚Äì4 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å —Ç–µ–≥–∞–º–∏ <i>‚Ä¶</i>. –ë–æ–ª—å—à–µ –Ω–∏–∫–∞–∫–∏—Ö HTML-—Ç–µ–≥–æ–≤."},
                {"role": "user", "content": f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n–¢–µ–∫—Å—Ç –¥–ª—è —Å–∂–∞—Ç–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏:\n{merged_text}"}
            ]
        }
        req = _url.Request(
            "https://api.openai.com/v1/chat/completions",
            data=_json.dumps(payload).encode("utf-8"),
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            method="POST",
        )
        with _url.urlopen(req, timeout=25) as resp:
            data = _json.loads(resp.read().decode("utf-8"))
        t = data["choices"][0]["message"]["content"].strip()
        # –†–∞–∑—Ä–µ—à–∞–µ–º —Ç–æ–ª—å–∫–æ <i>‚Ä¶</i>, –æ—Å—Ç–∞–ª—å–Ω–æ–µ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º.
        t = t.replace("\r", "")
        t_esc = html.escape(t)
        t_esc = t_esc.replace("&lt;i&gt;", "<i>").replace("&lt;/i&gt;", "</i>")
        return t_esc
    except Exception:
        return None


# ---------------- Telegram ----------------
def tg_send_photo(token: str, chat_id: str, caption_html: str, photo_url: Optional[str], thread_id: Optional[int] = None) -> int:
    url = f"{TELEGRAM_API_BASE}/bot{token}/sendPhoto"
    data = {"chat_id": chat_id, "caption": caption_html, "parse_mode": "HTML"}
    if thread_id is not None:
        data["message_thread_id"] = thread_id
    if photo_url:
        data["photo"] = photo_url
    r = requests.post(url, data=data, timeout=25)
    r.raise_for_status()
    return r.json()["result"]["message_id"]


def tg_copy(token: str, from_chat: str, msg_id: int, to_chat: str):
    url = f"{TELEGRAM_API_BASE}/bot{token}/copyMessage"
    data = {"from_chat_id": from_chat, "message_id": msg_id, "chat_id": to_chat}
    requests.post(url, data=data, timeout=20)


# ---------------- Main ----------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--url", required=True)
    ap.add_argument("--item-selector", required=True)
    ap.add_argument("--limit", type=int, default=6)
    ap.add_argument("--base-url")
    ap.add_argument("--state", default=STATE_FILE)
    ap.add_argument("--with-photo", action="store_true")
    ap.add_argument("--thread-id", type=int)
    ap.add_argument("--copy-to-chat-id")
    args = ap.parse_args()

    token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
    chat_id = os.getenv("TELEGRAM_CHAT_ID", "").strip()
    if not token or not chat_id:
        print("Set TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID", file=sys.stderr); sys.exit(2)

    # —Ç–µ–º–∞/–∫–æ–ø–∏—è
    thread_id = args.thread_id
    if thread_id is None:
        thread_env = os.getenv("TELEGRAM_THREAD_ID", "").strip()
        if thread_env.isdigit():
            thread_id = int(thread_env)
    copy_chat = os.getenv("TELEGRAM_COPY_TO_CHAT_ID", "").strip() or (args.copy_to_chat_id or "").strip()

    # —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
    listing = fetch_html(args.url)
    links = extract_listing_links(listing, args.base_url, args.item_selector, args.limit)

    # state
    seen = set()
    if os.path.exists(args.state):
        try:
            seen = set(json.load(open(args.state, "r", encoding="utf-8")))
        except Exception:
            seen = set()

    posted = 0
    for link in links:
        uid = link  # URL –∫–∞–∫ id
        if uid in seen:
            continue

        title, image, paras = parse_article(link, args.base_url)
        base_text = medium_text(paras, target=700)

        # LLM-–ø–æ–≤–∞—Ä –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—É—Ä—Å–∏–≤
        cooked = llm_make_text(title, base_text) or italicize_some_keywords(base_text)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ caption (–±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫!)
        emoji = choose_emoji(title, base_text)
        cap_parts = [
            f"{emoji} <b>{html.escape(title)}</b>",
            cooked,
            "üèéÔ∏è *–†—É–ª–Å–∂–∫–∞* (https://t.me/drive_hedgehog)",
        ]
        caption = "\n\n".join([p for p in cap_parts if p]).strip()
        if len(caption) > 1024:
            caption = caption[:1000].rstrip() + "‚Ä¶\n\nüèéÔ∏è *–†—É–ª–Å–∂–∫–∞* (https://t.me/drive_hedgehog)"

        msg_id = tg_send_photo(token, chat_id, caption, image if args.with_photo else None, thread_id)
        if copy_chat:
            tg_copy(token, chat_id, msg_id, copy_chat)

        seen.add(uid)
        with open(args.state, "w", encoding="utf-8") as f:
            json.dump(sorted(list(seen)), f, ensure_ascii=False, indent=2)

        posted += 1
        time.sleep(1.1)

    print(f"Done. Posted {posted} item(s).")


if __name__ == "__main__":
    main()
