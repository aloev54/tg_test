#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
site_to_telegram.py ‚Äî –†—É–ª–Å–∂–∫–∞-—Å—Ç–∞–π–ª (LLM, —Ñ–æ—Ç–æ –∏ —Ç–µ–∫—Å—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏)

–ü—Ä–∞–≤–∏–ª–∞:
‚Ä¢ –ü–∏—à–∏ –∂–∏–≤–æ –∏ —Å —É–≤–ª–µ—á–µ–Ω–∏–µ–º ‚Äî –∫–∞–∫ –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—å.
‚Ä¢ –ù–∞—á–∏–Ω–∞–π —Å –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ–≥–æ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è.
‚Ä¢ –î–∞–ª—å—à–µ ‚Äî —Ü–∏—Ñ—Ä—ã/—Ñ–∞–∫—Ç—ã/–¥–µ—Ç–∞–ª–∏, –Ω–æ –±–µ–∑ —Å—É—Ö–æ—Å—Ç–∏; –Ω–µ–º–Ω–æ–≥–æ —ç–º–æ—Ü–∏–π/—Å—Ä–∞–≤–Ω–µ–Ω–∏–π.
‚Ä¢ –§–æ—Ä–º–∞—Ç: Markdown-—Å—Ç–∏–ª—å (**–∂–∏—Ä–Ω—ã–π**, *–∫—É—Ä—Å–∏–≤*), –Ω–æ –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ HTML (<b>, <i>).
‚Ä¢ –≠–º–æ–¥–∑–∏-–Ω—É–º–µ—Ä–∞—Ü–∏—è 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π, –±–µ–∑ –æ–±—ã—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤.
‚Ä¢ –í –∫–æ–Ω—Ü–µ ‚Äî –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º—ã—Å–ª—å (–±–µ–∑ —Å–ª–æ–≤–∞ ¬´–≤—ã–≤–æ–¥¬ª).
‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –≤—Å—Ç–∞–≤–ª—è—Ç—å.
‚Ä¢ –§–æ—Ç–æ –∏ —Ç–µ–∫—Å—Ç ‚Äî –û–¢–î–ï–õ–¨–ù–´–ú–ò —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏.
‚Ä¢ –ü–æ–¥–ø–∏—Å—å –≤–Ω–∏–∑—É —Ç–µ–∫—Å—Ç–∞: üèéÔ∏è <a href="https://t.me/drive_hedgehog">–†—É–ª–Å–∂–∫–∞</a>

–ü–æ–¥–¥–µ—Ä–∂–∫–∞:
‚Ä¢ message_thread_id (—Ç–µ–º—ã –≤ –≥—Ä—É–ø–ø–µ)
‚Ä¢ –∫–æ–ø–∏—è –≤ –¥—Ä—É–≥–æ–π —á–∞—Ç/–∫–∞–Ω–∞–ª (–∫–æ–ø–∏—Ä—É–µ–º –∏ —Ñ–æ—Ç–æ, –∏ —Ç–µ–∫—Å—Ç)
"""

import argparse, html, json, os, re, sys, time
from dataclasses import dataclass
from typing import List, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

DEFAULT_UA = "Mozilla/5.0 (compatible; rul-ezhka/2.1)"
TELEGRAM_API_BASE = "https://api.telegram.org"
STATE_FILE = "autonews_seen_nb.json"

ARTICLE_SELECTORS = [
    "article","[itemprop='articleBody']",".article__body",".js-mediator-article",".article",
]
DROP_SELECTORS = [
    "script","style","noscript",
    "[class*='advert']","[class*='ad-']","[class*='ad_']","[id*='ad']",
    "[class*='banner']","[class*='promo']",
    "[class*='subscribe']","[class*='subscription']",
    "[class*='breadcrumbs']","[class*='share']","[class*='social']",
    "[class*='tags']","[class*='related']","[class*='widget']",
    "figure figcaption",".photo-credit",".copyright",
]
DROP_PHRASES = [
    "—Ä–µ–∫–ª–∞–º–∞","–ø–æ–¥–ø–∏—Å","—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ","—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ","–Ω–∞—à —Ç–µ–ª–µ–≥—Ä–∞–º",
    "–ø—Ä–æ–º–æ–∫–æ–¥","–ø–æ–¥—Ä–æ–±–Ω–µ–µ","—Ä–µ–∫–ª–∞–º","—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ","—Å–∫–∞—á–∞–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ",
]

EMOJI_MAP = [
    (["–¥—Ç–ø","–∞–≤–∞—Ä","—Å—Ç–æ–ª–∫–Ω–æ–≤"],"üö®"),
    (["—à—Ç—Ä–∞—Ñ","–Ω–∞–ª–æ–≥","–ø–æ—à–ª–∏–Ω","—É—Ç–∏–ª—å—Å–±–æ—Ä"],"üí∏"),
    (["—ç–ª–µ–∫—Ç—Ä–æ","ev","–±–∞—Ç–∞—Ä","–∑–∞—Ä—è–¥"],"‚ö°"),
    (["–±–µ–Ω–∑–∏–Ω","–¥–∏–∑–µ–ª","—Ç–æ–ø–ª–∏–≤"],"‚õΩ"),
    (["—Ç—Ä–∞—Å—Å","–¥–æ—Ä–æ–≥","—Ä–µ–º–æ–Ω—Ç","–º–æ—Å—Ç"],"üõ£Ô∏è"),
    (["–≥–æ–Ω–∫","—Å–ø–æ—Ä—Ç","—Ä–∞–ª–ª–∏","—Ç—Ä–µ–∫"],"üèÅ"),
]

@dataclass
class Item:
    title: str
    url: str
    image: Optional[str]
    paras: List[str]

# ---------- fetch & parse ----------
def fetch_html(url:str)->str:
    r=requests.get(url,headers={"User-Agent":DEFAULT_UA},timeout=25)
    r.raise_for_status()
    return r.text

def normalize_title(title:str)->str:
    t=title.strip()
    patterns=[
        r"\s*[-‚Äì‚Äî|:]{1,3}\s*(–ì–ª–∞–≤–Ω–æ–µ\s*)?::?\s*Autonews(?:\.ru)?\s*$",
        r"\s*[-‚Äì‚Äî|:]{1,3}\s*Autonews(?:\.ru)?\s*$",
        r"\s*\|\s*(–ì–ª–∞–≤–Ω–æ–µ|–ù–æ–≤–æ—Å—Ç–∏)\s*$",
        r"\s*::\s*(–ì–ª–∞–≤–Ω–æ–µ|–ù–æ–≤–æ—Å—Ç–∏)\s*$",
    ]
    for p in patterns:
        t=re.sub(p,"",t,flags=re.IGNORECASE)
    t=re.split(r"\s[-‚Äì‚Äî|:]{1,3}\s",t)[0].strip() or t
    return t

def extract_listing_links(html_text:str,base_url:Optional[str],selector:str,limit:int)->List[str]:
    soup=BeautifulSoup(html_text,"html.parser")
    nodes=soup.select(selector)[:limit]
    out=[]
    for n in nodes:
        a=n if n.name=="a" else n.find("a")
        if a and a.get("href"):
            href=urljoin(base_url,a["href"]) if base_url else a["href"]
            if href not in out:
                out.append(href)
    return out

def is_junk(t:str)->bool:
    lt=t.lower()
    if len(lt)<40: return True
    return any(p in lt for p in DROP_PHRASES)

def parse_article(url:str,base_url:Optional[str])->Item:
    soup=BeautifulSoup(fetch_html(url),"html.parser")

    # title
    t=soup.find("meta",property="og:title")
    title=t["content"].strip() if t and t.get("content") else (soup.title.string.strip() if soup.title else url)
    title=normalize_title(title)

    # image
    im=soup.find("meta",property="og:image")
    image=im["content"].strip() if im and im.get("content") else None
    if image and base_url: image=urljoin(base_url,image)

    # article body
    root=None
    for sel in ARTICLE_SELECTORS:
        node=soup.select_one(sel)
        if node: root=node; break
    if not root: root=soup

    # drop junk safely
    for sel in DROP_SELECTORS:
        try:
            for node in root.select(sel): node.decompose()
        except Exception: continue

    paras=[]
    for p in root.find_all("p"):
        txt=re.sub(r"\s+"," ",p.get_text(" ",strip=True))
        if txt and not is_junk(txt): paras.append(txt)

    if not paras:
        d=soup.find("meta",property="og:description")
        if d and d.get("content"): paras=[d["content"].strip()]

    return Item(title=title,url=url,image=image,paras=paras[:14])

# ---------- formatting helpers ----------
MD_BOLD = re.compile(r"\*\*(.+?)\*\*")
MD_ITAL = re.compile(r"(?<!\*)\*(?!\s)(.+?)(?<!\s)\*(?!\*)")

def markdown_to_html(md:str)->str:
    """–ü—Ä–æ—Å—Ç–æ–π –∫–æ–Ω–≤–µ—Ä—Ç: **bold** –∏ *italic* -> <b>, <i>. –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–µ."""
    # –°–Ω–∞—á–∞–ª–∞ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å—ë
    esc = html.escape(md)
    # –í–µ—Ä–Ω—ë–º –Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
    esc = MD_BOLD.sub(r"<b>\1</b>", esc)
    esc = MD_ITAL.sub(r"<i>\1</i>", esc)
    return esc

def strip_links(text:str)->str:
    # –≤—ã—á–∏—â–∞–µ–º –ª—é–±—ã–µ URL—ã, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –≤—Å—Ç–∞–≤–ª—è–ª–∞ —Å—Å—ã–ª–∫–∏
    return re.sub(r"https?://\S+", "", text)

def sanitize_bullets(text:str)->str:
    # —É–±–µ—Ä—ë–º –æ–±—ã—á–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã '-' –∏ '‚Ä¢' –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫
    return re.sub(r"^[\-\*‚Ä¢]\s+", "", text, flags=re.M)

def choose_emoji(title:str,text:str)->str:
    s=(title+" "+text).lower()
    for keys,e in EMOJI_MAP:
        if any(k in s for k in keys): return e
    return "üöó"

def join_text(paras:List[str],limit:int=1200)->str:
    out,cur=[],0
    for p in paras:
        if cur+len(p)>limit and out: break
        out.append(p);cur+=len(p)+1
        if len(out)>=7: break
    return " ".join(out)

# ---------- LLM ----------
def llm_style_post(title:str,text:str)->Optional[str]:
    """
    –ì–µ–Ω–µ—Ä–∏–º —Ç–µ–ª–æ –ø–æ—Å—Ç–∞ (–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞). Markdown-—Å—Ç–∏–ª—å: **–∂–∏—Ä–Ω—ã–π**, *–∫—É—Ä—Å–∏–≤*.
    –≠–º–æ–¥–∑–∏-–Ω—É–º–µ—Ä–∞—Ü–∏—è 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π. –ë–µ–∑ —Å—Å—ã–ª–æ–∫.
    """
    api=os.getenv("OPENAI_API_KEY","").strip()
    if not api: return None
    import json,urllib.request as urlreq
    sys_prompt=(
        "–¢—ã —Å–æ–∑–¥–∞—ë—à—å –ø–æ—Å—Ç –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª—è—Ö. "
        "–ü–∏—à–∏ –∂–∏–≤–æ –∏ —Å —É–≤–ª–µ—á–µ–Ω–∏–µ–º, –∫–∞–∫ –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—å, –∞ –Ω–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç. "
        "–°–Ω–∞—á–∞–ª–∞ 1‚Äì2 —Å—Ç—Ä–æ–∫–∏ –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ–≥–æ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è, –∑–∞—Ç–µ–º 3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤ —Ñ–∞–∫—Ç–æ–≤/–¥–µ—Ç–∞–ª–µ–π "
        "–¥–ª—è –ø—É–Ω–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏-–Ω—É–º–µ—Ä–∞—Ü–∏—é 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ (–¢–û–õ–¨–ö–û –î–õ–Ø –ü–£–ù–ö–¢–û–í, –û–°–¢–ê–õ–¨–ù–û–ï –ü–†–û–°–¢–û –¢–ï–ö–°–¢!!!). "
        "–î–æ–±–∞–≤–ª—è–π —Ü–∏—Ñ—Ä—ã –∏ –¥–µ—Ç–∞–ª–∏, –Ω–æ –±–µ–∑ —Å—É—Ö–æ—Å—Ç–∏; –Ω–µ–º–Ω–æ–≥–æ —ç–º–æ—Ü–∏–π, —Å—Ä–∞–≤–Ω–µ–Ω–∏–π –∏–ª–∏ –º–µ—Ç–∞—Ñ–æ—Ä. "
        "–ò—Å–ø–æ–ª—å–∑—É–π Markdown: **–∂–∏—Ä–Ω—ã–π** –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤, *–∫—É—Ä—Å–∏–≤* –¥–ª—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π. "
        "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—ã—á–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã '-', '‚Ä¢'. –°—Å—ã–ª–∫–∏ –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π. "
        "–ó–∞–≤–µ—Ä—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º—ã—Å–ª—å—é/—Å–æ–≤–µ—Ç–æ–º –±–µ–∑ —Å–ª–æ–≤–∞ ¬´–≤—ã–≤–æ–¥¬ª. "
        "–í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞."
    )
    payload={
        "model":"gpt-4o-mini",
        "temperature":0.4,
        "messages":[
            {"role":"system","content":sys_prompt},
            {"role":"user","content":f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –æ—Å–Ω–æ–≤—ã:\n{text}"}
        ]
    }
    req=urlreq.Request(
        "https://api.openai.com/v1/chat/completions",
        data=json.dumps(payload).encode("utf-8"),
        headers={"Authorization":f"Bearer {api}","Content-Type":"application/json"},
        method="POST",
    )
    try:
        with urlreq.urlopen(req,timeout=30) as resp:
            d=json.loads(resp.read().decode("utf-8"))
        md = d["choices"][0]["message"]["content"].strip().replace("\r","")
        md = strip_links(md)
        md = sanitize_bullets(md)
        return markdown_to_html(md)
    except Exception:
        return None

# ---------- Telegram ----------
def tg_send_photo(token:str,chat_id:str,photo_url:str,thread_id:Optional[int]=None)->int:
    data={"chat_id":chat_id,"photo":photo_url}
    if thread_id is not None: data["message_thread_id"]=thread_id
    r=requests.post(f"{TELEGRAM_API_BASE}/bot{token}/sendPhoto",data=data,timeout=30)
    r.raise_for_status()
    return r.json()["result"]["message_id"]

def tg_send_text(token:str,chat_id:str,text_html:str,thread_id:Optional[int]=None)->int:
    data={"chat_id":chat_id,"text":text_html,"parse_mode":"HTML","disable_web_page_preview":True}
    if thread_id is not None: data["message_thread_id"]=thread_id
    r=requests.post(f"{TELEGRAM_API_BASE}/bot{token}/sendMessage",data=data,timeout=30)
    r.raise_for_status()
    return r.json()["result"]["message_id"]

def tg_copy(token:str,from_chat_id:str,message_id:int,to_chat_id:str)->None:
    requests.post(f"{TELEGRAM_API_BASE}/bot{token}/copyMessage",
                  data={"from_chat_id":from_chat_id,"message_id":message_id,"chat_id":to_chat_id},
                  timeout=20)

# ---------- Main ----------
def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--url",required=True)
    ap.add_argument("--item-selector",required=True)
    ap.add_argument("--limit",type=int,default=1)
    ap.add_argument("--base-url")
    ap.add_argument("--state",default=STATE_FILE)
    ap.add_argument("--with-photo",action="store_true")
    ap.add_argument("--thread-id",type=int)
    ap.add_argument("--copy-to-chat-id")
    a=ap.parse_args()

    token=os.getenv("TELEGRAM_BOT_TOKEN","").strip()
    chat=os.getenv("TELEGRAM_CHAT_ID","").strip()
    if not token or not chat:
        sys.exit("Need TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID")

    env_th=os.getenv("TELEGRAM_THREAD_ID","").strip()
    if a.thread_id is not None:
        th=a.thread_id
    elif env_th.isdigit():
        th=int(env_th)
    else:
        th=None

    copy=os.getenv("TELEGRAM_COPY_TO_CHAT_ID","").strip() or (a.copy_to_chat_id or "").strip()

    # state
    seen=set()
    if os.path.exists(a.state):
        try:
            with open(a.state,"r",encoding="utf-8") as f:
                seen=set(json.load(f))
        except Exception:
            seen=set()

    # listing
    listing=fetch_html(a.url)
    links=extract_listing_links(listing,a.base_url,a.item_selector,a.limit)

    for link in links:
        if link in seen: continue
        item=parse_article(link,a.base_url)
        merged=join_text(item.paras,limit=1200)

        # —Ç–µ–∫—Å—Ç –æ—Ç LLM –∏–ª–∏ fallback-–ø—Ä–æ—Å—Ç–æ–π —Å–∫–ª–µ–π–∫–∏ –±–µ–∑ –º–∞—Ä–∫–µ—Ä–æ–≤
        body_html = llm_style_post(item.title, merged)
        if not body_html:
            # fallback: –∏–Ω—Ç—Ä–æ + 1Ô∏è‚É£2Ô∏è‚É£3Ô∏è‚É£ –∏–∑ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            sents=re.split(r"(?<=[.!?‚Ä¶])\s+", merged)
            intro=" ".join(sents[:2]).strip()
            pts=[s for s in sents[2:] if 50 <= len(s) <= 220][:4]
            emojis=["1Ô∏è‚É£","2Ô∏è‚É£","3Ô∏è‚É£","4Ô∏è‚É£"]
            md = intro + ("\n" if intro and pts else "") + "\n".join(f"{emojis[i]} {pts[i]}" for i in range(len(pts)))
            md = sanitize_bullets(strip_links(md))
            body_html = markdown_to_html(md)

        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–ó–ê–ì–û–õ–û–í–û–ö + —Ç–µ–ª–æ + –ø–æ–¥–ø–∏—Å—å)
        emoji=choose_emoji(item.title, merged)
        header = f"{emoji} <b>{html.escape(item.title)}</b>"
        signature = "üèéÔ∏è <a href=\"https://t.me/drive_hedgehog\">–†—É–ª–Å–∂–∫–∞</a>"
        text_html = f"{header}\n\n{body_html}\n\n{signature}"

        # –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –û–¢–î–ï–õ–¨–ù–û: —Ñ–æ—Ç–æ (–±–µ–∑ caption) ‚Üí —Ç–µ–∫—Å—Ç
        msg_photo_id = None
        if a.with_photo and item.image:
            msg_photo_id = tg_send_photo(token, chat, item.image, th)
        msg_text_id = tg_send_text(token, chat, text_html, th)

        # –∫–æ–ø–∏—Ä—É–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∏ —Ñ–æ—Ç–æ, –∏ —Ç–µ–∫—Å—Ç)
        if copy:
            if msg_photo_id: tg_copy(token, chat, msg_photo_id, copy)
            tg_copy(token, chat, msg_text_id, copy)

        # state
        seen.add(link)
        with open(a.state,"w",encoding="utf-8") as f:
            json.dump(sorted(list(seen)),f,ensure_ascii=False,indent=2)

        time.sleep(1.0)

    print("Done.")

if __name__=="__main__":
    main()
